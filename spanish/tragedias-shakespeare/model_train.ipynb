{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open('tragedias_shakespeare_filtered.txt') as file:\n",
    "    text = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "' con sus espadas.\\nYo soy el primogénito del último\\nque portó la diadema imperial de Roma;\\ndejen pues que el honor de mi padre viva en mí,\\ny no ultrajen mi mayorazgo con este agravio.\\nBASIANO (A sus seguidores .)\\nRomanos, amigos, seguidores,\\ndefensores de mis derechos,\\nsi alguna vez Basiano, hijo de César,\\nfue visto con gracia por la real Roma,\\ncuiden entonces este paso al Capitolio,\\ny no padezcan el deshonor de acercarse\\nal trono imperial, consagrado a la virtud,\\nla justicia, la continencia, la nobleza;\\ndejen que el mérito brille en elección transparente,\\n\\x0cy, romanos, peleen por su libertad de'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[400:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "1410105"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "len(encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    one_hot_array = np.zeros((np.multiply(*arr.shape),n_labels),dtype=np.float32)\n",
    "\n",
    "    one_hot_array[np.arange(one_hot_array.shape[0]),arr.flatten()] = 1.\n",
    "\n",
    "    one_hot_array = one_hot_array.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make, // integer division, round down\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "\n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows, n. of first row is the batch size, the other lenght is inferred\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[61 36 26 25 72 36  8 15 14 68]\n",
      " [45  8 82 45 39 70  7 82 37 82]\n",
      " [37  8 88  7 84 44 63 82  7 37]\n",
      " [45 39  8 46 37 39  8 70 37 39]\n",
      " [71 56 19 46 37 35 37 54  8 37]\n",
      " [52 48 63  8 54 83 45 82 35 37]\n",
      " [44 37 46 45  8 84 33 35 63  8]\n",
      " [45  8 37 34 83 59 71 56 72 69]]\n",
      "\n",
      "y\n",
      " [[36 26 25 72 36  8 15 14 68 51]\n",
      " [ 8 82 45 39 70  7 82 37 82 43]\n",
      " [ 8 88  7 84 44 63 82  7 37 24]\n",
      " [39  8 46 37 39  8 70 37 39 37]\n",
      " [56 19 46 37 35 37 54  8 37 46]\n",
      " [48 63  8 54 83 45 82 35 37 39]\n",
      " [37 46 45  8 84 33 35 63  8 44]\n",
      " [ 8 37 34 83 59 71 56 72 69 44]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}