{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIMER ACTO\n",
      "\n",
      "\n",
      "ESCENA I\n",
      "\n",
      "Fanfarria . Entran arriba los tribunos y los senadores; luego, abajo, por\n",
      "una puerta SATURNINO y sus seguidores y por otra BASIANO y los\n",
      "suyos, con tambores y estandartes .\n",
      "\n",
      "SATURNINO: (A sus seguidores .)\n",
      "Nobles patricios, protectores de mis derechos,\n",
      "defiendan la verdad de mi causa con las armas;\n",
      "y ustedes, compatriotas, mis amados seguidores,\n",
      "aboguen por mi título de suc\n"
     ]
    }
   ],
   "source": [
    "with open('final_dataset.txt') as file:\n",
    "    TEXT = file.read()\n",
    "\n",
    "print(TEXT[:400])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "EPOCHS = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make, // integer division, round down\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "\n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows, n. of first row is the batch size, the other lenght is inferred\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    one_hot_array = np.zeros((np.multiply(*arr.shape),n_labels),dtype=np.float32)\n",
    "\n",
    "    one_hot_array[np.arange(one_hot_array.shape[0]),arr.flatten()] = 1.\n",
    "\n",
    "    one_hot_array = one_hot_array.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "chars = tuple(set(TEXT))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in TEXT])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[56 81 74  6 93 81 79 68 91 36]\n",
      " [59 28 76 32 79 53 60 76 32 94]\n",
      " [79  5 94  2 85 60  4 35 25 47]\n",
      " [94 32 76  8 40 94 92  2  5 79]\n",
      " [79 25 94 32 79  7 28 62  8 10]\n",
      " [38 62 79 62 25 79 28 62 38 94]\n",
      " [94 28 58 60 76 79 32 76 79  4]\n",
      " [ 8 76  8 47 94 79 68  7 28 47]]\n",
      "\n",
      "y\n",
      " [[81 74  6 93 81 79 68 91 36 73]\n",
      " [28 76 32 79 53 60 76 32 94 32]\n",
      " [ 5 94  2 85 60  4 35 25 47 28]\n",
      " [32 76  8 40 94 92  2  5 79 25]\n",
      " [25 94 32 79  7 28 62  8 10 47]\n",
      " [62 79 62 25 79 28 62 38 94  8]\n",
      " [28 58 60 76 79 32 76 79  4 60]\n",
      " [76  8 47 94 79 68  7 28 47 35]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4,\n",
    "                 drop_prob=0.5, lr=0.001,batch_size=64):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.train_on_gpu = torch.cuda.is_available()\n",
    "        self.epochs_trained=0\n",
    "\n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        self.encoded = np.array([self.char2int[ch] for ch in TEXT])\n",
    "\n",
    "    ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (self.train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "    def model_train(self, epochs=1, seq_length=50, clip=5, val_frac=0.1, print_every=10,print_state=True):\n",
    "        ''' Training a network\n",
    "\n",
    "            Arguments\n",
    "            ---------\n",
    "\n",
    "            net: CharRNN network\n",
    "            data: text data to train the network\n",
    "            epochs: Number of epochs to train\n",
    "            batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "            seq_length: Number of character steps per mini-batch\n",
    "            lr: learning rate\n",
    "            clip: gradient clipping\n",
    "            val_frac: Fraction of data to hold out for validation\n",
    "            print_every: Number of steps for printing training and validation loss\n",
    "\n",
    "        '''\n",
    "        loss_arr = []\n",
    "        val_loss_arr = []\n",
    "        if(self.train_on_gpu):\n",
    "            self.cuda()\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # create training and validation data\n",
    "        val_idx = int(len(self.encoded)*(1-val_frac))\n",
    "        data, val_data = self.encoded[:val_idx], self.encoded[val_idx:]\n",
    "\n",
    "\n",
    "\n",
    "        counter = 0\n",
    "        n_chars = len(self.chars)\n",
    "        for e in range(epochs):\n",
    "            # initialize hidden state\n",
    "            h = self.init_hidden(self.batch_size)\n",
    "\n",
    "            for x, y in get_batches(data, self.batch_size, seq_length):\n",
    "                counter += 1\n",
    "\n",
    "                # One-hot encode our data and make them Torch tensors\n",
    "                x = one_hot_encode(x, n_chars)\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                if(self.train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda().type(torch.cuda.LongTensor)\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                h = tuple([each.data for each in h])\n",
    "\n",
    "                # zero accumulated gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                # get the output from the model\n",
    "                output, h = self(inputs, h)\n",
    "\n",
    "                # calculate the loss and perform backprop\n",
    "                loss = criterion(output.cuda(), targets.view(self.batch_size*seq_length))\n",
    "                loss.backward()\n",
    "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "                opt.step()\n",
    "\n",
    "                # loss stats\n",
    "                if counter % print_every == 0 and print_state:\n",
    "                    # Get validation loss\n",
    "                    val_h = self.init_hidden(self.batch_size)\n",
    "                    val_losses = []\n",
    "                    self.eval()\n",
    "                    for x, y in get_batches(val_data, self.batch_size, seq_length):\n",
    "                        # One-hot encode our data and make them Torch tensors\n",
    "                        x = one_hot_encode(x, n_chars)\n",
    "                        x, y = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
    "\n",
    "                        # Creating new variables for the hidden state, otherwise\n",
    "                        # we'd backprop through the entire training history\n",
    "                        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                        inputs, targets = x, y\n",
    "                        if(self.train_on_gpu):\n",
    "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                        output, val_h = self(inputs, val_h)\n",
    "                        val_loss = criterion(output, targets.view(self.batch_size*seq_length).type(torch.cuda.LongTensor))\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                    self.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, self.epochs_trained),\n",
    "                          \"Step: {}...\".format(counter),\n",
    "                          \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                          \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                #val_loss_arr.append(val_loss.item())\n",
    "                loss_arr.append(loss.item())\n",
    "        return loss_arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.loss = 100\n",
    "\n",
    "    def define_model(self,trial):\n",
    "        n_layers = trial.suggest_int(\"n_layers\",4,6)\n",
    "        n_hidden = trial.suggest_int(\"n_hidden\",312,712,step=100)\n",
    "        drop_prob = trial.suggest_float(\"drop_prob\",0.4,0.6,step=0.05)\n",
    "        lr = trial.suggest_float(\"lr\",0.001,0.01,step=0.001)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",16,128,step=16)\n",
    "\n",
    "        return CharRNN(chars, n_hidden=n_hidden, n_layers=n_layers, drop_prob=drop_prob, lr=lr,batch_size=batch_size)\n",
    "\n",
    "    def objective(self,trial):\n",
    "        model = self.define_model(trial).to('cuda')\n",
    "        loss = 0\n",
    "        status = 'Current Params:\\n'\n",
    "        #print('Current Params:',end='\\r')\n",
    "        for key, value in trial.params.items():\n",
    "            status+=\"    {}: {}\".format(key, value) + '\\n'\n",
    "            #print(\"    {}: {}\".format(key, value),end='\\r')\n",
    "        print(status,end='\\r')\n",
    "\n",
    "        for epoch in tnrange(EPOCHS,desc='Internal Epochs'):\n",
    "            #print(\" ABSOLUTE EPOCH:   \"+str(epoch))\n",
    "            #print(\"     Relative Epochs:    \" + str(model.epochs_trained))\n",
    "            loss_arr= model.model_train(seq_length=160,print_state=False)\n",
    "            loss = loss_arr[0]\n",
    "            print(\"         Current Loss: %f\"%loss,end='\\r')\n",
    "            trial.report(loss,epoch)\n",
    "\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if loss < self.loss:\n",
    "            self.loss = loss\n",
    "            self.model = model\n",
    "        return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 18:20:12,295]\u001B[0m A new study created in memory with name: no-name-363f8683-9283-4ee4-8502-679f2c2a4473\u001B[0m\n",
      "C:\\ProgramData\\Anaconda3\\envs\\robo_shakespeare\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "636eabb91e3843918083b02803f49717"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.5\n",
      "    lr: 0.004\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1469911fcb84434dac186682e11da74c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 19:03:29,202]\u001B[0m Trial 0 finished with value: 3.285987377166748 and parameters: {'n_layers': 6, 'n_hidden': 612, 'drop_prob': 0.5, 'lr': 0.004, 'batch_size': 48}. Best is trial 0 with value: 3.285987377166748.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.008\n",
      "    batch_size: 96\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1992c897f47f45c4ae12974b32f61f41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 19:47:46,741]\u001B[0m Trial 1 finished with value: 3.30895733833313 and parameters: {'n_layers': 6, 'n_hidden': 612, 'drop_prob': 0.45, 'lr': 0.008, 'batch_size': 96}. Best is trial 0 with value: 3.285987377166748.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.01\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a19967b261ce4fe6afbd01ba54031332"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 20:27:34,205]\u001B[0m Trial 2 finished with value: 3.2844347953796387 and parameters: {'n_layers': 5, 'n_hidden': 612, 'drop_prob': 0.45, 'lr': 0.01, 'batch_size': 48}. Best is trial 2 with value: 3.2844347953796387.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.002\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4674b8e5927f4d9e99d721c83db3768b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 20:54:50,506]\u001B[0m Trial 3 finished with value: 0.9168450236320496 and parameters: {'n_layers': 4, 'n_hidden': 712, 'drop_prob': 0.4, 'lr': 0.002, 'batch_size': 32}. Best is trial 3 with value: 0.9168450236320496.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 512\n",
      "    drop_prob: 0.5\n",
      "    lr: 0.008\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fdd8840ab90441caee42658345d4bdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:07:01,541]\u001B[0m Trial 4 finished with value: 3.2884576320648193 and parameters: {'n_layers': 5, 'n_hidden': 512, 'drop_prob': 0.5, 'lr': 0.008, 'batch_size': 48}. Best is trial 3 with value: 0.9168450236320496.\u001B[0m Loss: 3.288988         Current Loss: 3.289417\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.55\n",
      "    lr: 0.01\n",
      "    batch_size: 80\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57c9115004344b50b444045c14cfc7ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:07:30,866]\u001B[0m Trial 5 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.6\n",
      "    lr: 0.002\n",
      "    batch_size: 80\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b1370d6efe1439d94b14ea0cb1ea8f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:08:07,430]\u001B[0m Trial 6 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 512\n",
      "    drop_prob: 0.6\n",
      "    lr: 0.008\n",
      "    batch_size: 16\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c11da5cc43f45d3bc81369ff7a8b63f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:08:55,152]\u001B[0m Trial 7 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 312\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.008\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff39dc2511644c87b769c7036577c52c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:09:28,126]\u001B[0m Trial 8 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 512\n",
      "    drop_prob: 0.6\n",
      "    lr: 0.01\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d85932fd0d75479f8a193344ca16330d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:10:03,375]\u001B[0m Trial 9 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.001\n",
      "    batch_size: 128\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79279122d71843acb94a82d258fcd357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:10:29,688]\u001B[0m Trial 10 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.004\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3871051cbde042a49017afb17aa1d43e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:10:58,911]\u001B[0m Trial 11 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.005\n",
      "    batch_size: 16\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccb2da1170e34198aa7e4e68b1fd536d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:12:27,012]\u001B[0m Trial 12 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 412\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.003\n",
      "    batch_size: 64\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c06db6562e148d9b99df20707c46abe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:12:36,733]\u001B[0m Trial 13 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.006\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af6b1435ab3045248aab53150a3e3284"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:13:09,680]\u001B[0m Trial 14 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.006\n",
      "    batch_size: 64\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88b5bfe03b024407a870a3e77e642048"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:13:41,131]\u001B[0m Trial 15 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 412\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.001\n",
      "    batch_size: 96\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dce6ba2b8fae4e2fb3e57df5ad53f157"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:13:50,540]\u001B[0m Trial 16 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.5\n",
      "    lr: 0.003\n",
      "    batch_size: 16\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ba68ab30ca940b3bdc71d783908e70e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:15:42,009]\u001B[0m Trial 17 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.55\n",
      "    lr: 0.007\n",
      "    batch_size: 128\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e827fb097a44bf1addde2dc3b7d52f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:16:10,288]\u001B[0m Trial 18 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.009000000000000001\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d259a519801d448c822e5d9fa8c1dcff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:17:19,284]\u001B[0m Trial 19 pruned. \u001B[0m\n",
      "Study statistics: \n",
      "  Number of finished trials:  20\n",
      "  Number of pruned trials:  15\n",
      "  Number of complete trials:  5\n",
      "Best trial:\n",
      "  Value:  0.9168450236320496\n",
      "  Params: \n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.002\n",
      "    batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluator = Objective()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(evaluator.objective,n_trials=20,show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(98, 712, num_layers=4, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=712, out_features=98, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "best_model = evaluator.model\n",
    "\n",
    "print(best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "n_hidden 612\n",
    "drop_prob 0.6\n",
    "lr 0.003"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "model_shakespeare_tragedias = 'rnn_50_epoch_4-712_batch-32_lr-002.net'\n",
    "\n",
    "checkpoint = {'n_hidden': best_model.n_hidden,\n",
    "              'n_layers': best_model.n_layers,\n",
    "              'state_dict': best_model.state_dict(),\n",
    "              'tokens': best_model.chars}\n",
    "\n",
    "with open(model_shakespeare_tragedias, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "    ''' Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "    '''\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    # apply softmax to get p probabilities for the likely next character giving x\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu() # move to cpu\n",
    "\n",
    "    # get top characters\n",
    "    # considering the k most probable characters with topk method\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return net.int2char[char], h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def sample(net, size, prime='PRIMER ACTO', top_k=None):\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval() # eval mode\n",
    "\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué ta chendo? ¡Sexo,sexo! ¡Ahora que\n",
      "este es el mal con las piedras que mi antiguo encuentro\n",
      "para ser servido!\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA VII\n",
      "\n",
      "Un palacio.\n",
      "Entran LENNOX, ROSS\n",
      "y AARÓN, cuanto la puerta .\n",
      "\n",
      "Sale .\n",
      "\n",
      "COMINIO: ¿Quién es el dolor de su señor?\n",
      "\n",
      "ANTONIO: Así es ahí tienes.\n",
      "\n",
      "SEGUNDA: BRUJA ¡Maldito se acerca!\n",
      "\n",
      "Salen el MÉDICO y CAFIS.\n",
      "\n",
      "ANTONIO: ¡Ay, estas señal e informes!\n",
      "\n",
      "SOLDADO: SEGUNDO No, dijo que se alimentará pensarlo.\n",
      "Santo espeso del cielo, este amor que se la habría sucedido.\n",
      "\n",
      "LADY: MACBETH ¿Dónde está el modo? ¡Socorro!\n",
      "¿No es la verdad de mi amigo, como linaje,\n",
      "según se estrecha en la tercera cabeza? ¿Qué hace, y su amigo?\n",
      "\n",
      "ROMEO: ¿Qué? ¿Por qué natural donde el mismo\n",
      "dulce sabiduría en su curso\n",
      "el poder derribero en mi muerte a las cuerpas? ¿Cómo es un asesinato\n",
      "que así soy la cama el punto de esos palabras\n",
      "como un monto, con medio de los cuernos, con las prostiguidas\n",
      "en la nada, cual caballos camino aquí con la muerte?\n",
      "¿No te he vivido esos agravios alcanzados\n",
      "al menos capacer como un saludo de su padre\n",
      "y como el agua de la maldad se enviaría?\n",
      "\n",
      "CORDELIA: ¿Cómo, cuándo empezó?\n",
      "\n",
      "Sale el CABALLERO.\n",
      "¿Qué hizo particular de la verdigo?\n",
      "\n",
      "LEAR: No.\n",
      "\n",
      "ANTONIO: ¡Ah, sin darle a mí! ¿Con qué me presenta la trampa?\n",
      "\n",
      "ANTONIO: No lo eristarías.\n",
      "Estoy de loco, señor mío,\n",
      "sin duda alguna de tu amor presente.\n",
      "\n",
      "ANTONIO: ¡Mientras lo tienes a su amor!\n",
      "\n",
      "EMILIA: Señor, señor, solo sufres de lugar y cuerno desde la muerte\n",
      "es estrechamente.\n",
      "\n",
      "LUCILIO: A tu señora me perden a esto, pero es el corazón que doy, y se dice\n",
      "un confundido cual a las calles.\n",
      "¿Está sabido? ¡Oh, males, seguro!\n",
      "\n",
      "ALBANY: Aquí estoy, bien.\n",
      "\n",
      "CHARMIAN: No, si te has confiará esa propia pataniza, no es más de que te\n",
      "conozco. ¿Qué se arrastrará de un miedo exparcharás e incendicionero,\n",
      "para arrancar su castigo?\n",
      "\n",
      "SOLDADO: Así sea.\n",
      "Mira el mar y eso.\n",
      "\n",
      "ENOBARBO: ¿Cómo? ¿Por qué no te he oído?\n",
      "\n",
      "SIRVIENTE: DE FILOTO ¿Cómo? ¿No es la mista muerte del cielo\n",
      "que las preservas están aquí, a su sangre?\n",
      "¿Qué sucede? ¡Ay, despachado!\n",
      "\n",
      "Sale CORDELIA.\n",
      "\n",
      "Entra CLEOPATRA.\n",
      "¿Y si este mar me propongo?\n",
      "\n",
      "BRUTO: ¿Ello te habría cometido a los hombres?\n",
      "Entre el dulce modo en mi acaso siga a su sangre.\n",
      "¿No pueden acuñir a sus cartos?\n",
      "\n",
      "SENADOR: PRIMERO ¿Puede ahora?\n",
      "\n",
      "APEMANTO: ¡Oh, señora, encontrarás estos preparativos dispuestos!\n",
      "¡Oh, maldito es el mejor cenal en una corona?\n",
      "\n",
      "CAPULETO: ¿Que se acorreará en esto así? Ahí está\n",
      "si en tus cabezas me parecen para que tengo esperanza.\n",
      "\n",
      "LUCIO: Aquí, señor, el cielo el de la parte de su padre.\n",
      "Sus hijos alconcrean con sus mujeres y sacrificios\n",
      "de su parte, a la cual se enfría aquí.\n",
      "\n",
      "LEAR: ¿Es este ese el padre de César?\n",
      "\n",
      "BALTASAR: ¡Pobre Tom!\n",
      "\n",
      "BRUTO: No me había honro. ¿De eso has escapado así?\n",
      "¿Qué pretende siempre? ¡Oh, muerte, sus angustias!\n",
      "¿Pues es por lo que este tirano malas muertos?\n",
      "\n",
      "ROMEO: A mí no me da algo. ¡Ah, qué digno de secuel\n",
      "despierta.\n",
      "Es este mar deber de tonto presto.\n",
      "\n",
      "Le dura con las puertas .\n",
      "\n",
      "ROSENCRANTZ: ¿De veras? No, no te ataremos.\n",
      "\n",
      "SEGUNDO: SENADOR ¿Es que no te atañas?\n",
      "\n",
      "AAR:ÓN ¿Quién me ha hecho mañana?\n",
      "\n",
      "SOLDADO: PRIMERO Si les encuentra lujuria, para cambiado disfruto.\n",
      "CÉSAR ¿Y si es como la puerta?\n",
      "\n",
      "CASIO: ¿No te aconsejo eso?\n",
      "¿Callar a ustedes, mi buen señor?\n",
      "¿Por qué hemos sufrido contra mí el duque?\n",
      "\n",
      "ANTONIO: ¿Qué pasa?\n",
      "\n",
      "SIRVIENTE: DE VARRÓN No, señora.\n",
      "\n",
      "POLONIO: Sí; es el cepo. ¡Qué pie a esto!\n",
      "\n",
      "Sale .\n",
      "Entra LUCIANO.\n",
      "\n",
      "ANTONIO: ¿Con eso?\n",
      "\n",
      "CLEOPATRA: ¡Adiós! Ahora vuestra especie de los hermanos.\n",
      "Pero si lo que son enseguidas. ¿Qué hay?\n",
      "\n",
      "CORNWALL: ¡Adiós!\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA VI\n",
      "\n",
      "Entran los señores, por campos de sus amomentos cuando el cuerpo de Antonio .\n",
      "\n",
      "CLITO: Ahora te haga aquí.\n",
      "\n",
      "CASIO: No, milord.\n",
      "\n",
      "HAMLET: Señor, más que la muerte\n",
      "del asunto este conflicto\n",
      "que ha estado en ellos. Señor mío,\n",
      "que nuestro amor nos propondrá\n",
      "que enseñarle en esto antes que lo hondo es cierto,\n",
      "sino desde que el mucho que escapó del mundo,\n",
      "que no era el dios, y a mi alma y mis cartas\n",
      "cuando la separaba alcanzan\n",
      "los ojos; y lo que se acerca\n",
      "con su estrella disputa en su perro y circunstancia.\n",
      "¿Qué es lo que es esto?\n",
      "\n",
      "SATURNINO: (Aparte .) ¡Ah, qué, amigo y ver ahora\n",
      "el deseo que las venganza!\n",
      "\n",
      "CASIO: ¿Qué pasa?\n",
      "\n",
      "CALPURNIA: ¿Quise ser suya?\n",
      "\n",
      "SOLDADO: SEGUNDO ¿Cuánto hace que no es más que tu padre,\n",
      "que no es así?\n",
      "Soy una cara más perdería preferiría mí el amor\n",
      "por ella al cuerpo de listo, y estas alegres\n",
      "maldiciones y les escuento a sus pasiones,\n",
      "y a los deseos, de su poder en tierra.\n",
      "¿Qué sucede?\n",
      "\n",
      "RODRIGO: No enviaré a llegar.\n",
      "\n",
      "LEAR: ¡Que tu señor me alabe, con la campana de las tierra!\n",
      "\n",
      "SE:ÑORA CAPULETO ¿Quién me arrodillo? ¿Cómo estás?\n",
      "\n",
      "CLEOPATRA: Sí, por mi amo de ser tratas,\n",
      "y esto muy bien perderoso,\n",
      "ya que el de la corte está muerta. Su honor me despido.\n",
      "Serás le considero al marido en la castillón.\n",
      "\n",
      "Se arramiaba .\n",
      "¡Algo!\n",
      "\n",
      "Entra CASIO, con un CRIADO.\n",
      "Ah, señora.\n",
      "\n",
      "CLEOPATRA: No es ponderezcado este anciano.\n",
      "¿Qué has visto?\n",
      "\n",
      "ANTONIO: No hice algo de mala serio.\n",
      "¿No te esconde en mí, el de mis dos?\n",
      "\n",
      "MACBETH: ¡Ah, señor, es este miedo escapa y de ella\n",
      "y el primero, y espero de su cravesta!\n",
      "¿Pero eres mi amor este contento más pobre?\n",
      "\n",
      "ROMEO: Sí, mi amo.\n",
      "\n",
      "CLEOPATRA: Señora. ¡Qué su medicina!\n",
      "Si hubiera sido mañana está mi madre,\n",
      "y a tu madre sean las tiendas,\n",
      "y se aposta a estos maldecirios.\n",
      "\n",
      "CLEOPATRA: ¿Cuál es mi amo del turco?\n",
      "\n",
      "CLEOPATRA: ¿Qué perdió a mi alma, que no hubiese\n",
      "de senter aquí a su mujer?\n",
      "\n",
      "CLEOPATRA: ¡Salve, escuche el cielo sus asesinos,\n",
      "y el papel que solo es lo mejor,\n",
      "y la circunstancia se envenenó a mi padre\n",
      "y a tu muralla es más que el del cielo;\n",
      "pero su pellego hace a matarme.\n",
      "\n",
      "ANTONIO: Ahora venga a mi apresura.\n",
      "¿Que nos decístelo en su pensar?\n",
      "\n",
      "SEGUNDO: ACTO\n",
      "\n",
      "\n",
      "ESCENA I\n",
      "\n",
      "Entra FRAY LORENZO.\n",
      "\n",
      "PRIMER: ASESINO Señora.\n",
      "\n",
      "CLEOPATRA: ¡Callad, señor, para que se te ha dejo\n",
      "algún amargo, después, aunque el desprecio honrado\n",
      "descubre a un solo padre en mi contallente\n",
      "empalatante escoltar a César!\n",
      "\n",
      "TITO: ¿Que me atreves, cobra a ser el aguijón, ese mejor que todo\n",
      "acampar a su alcoba o con la pestilente de los amos? El modo soporto\n",
      "el mundo en la trampa de la presencia de mi señora.\n",
      "\n",
      "CORNWALL: Así como la marea de la mirada de los ojos.\n",
      "\n",
      "CHARMIAN: No hay amigos míos, eso, después de este hombre.\n",
      "Cuando esté es mi propia sangre.\n",
      "¿Por qué el mundo ha dado el maldito casado?\n",
      "¿Qué es esto son los hombres, cuando se estremece tiempo?\n",
      "¿Dónde va con los ojos?\n",
      "\n",
      "CORNWALL: Sin embargo, me perdido.\n",
      "¿Dónde tienes cuernos de mis campanas?\n",
      "\n",
      "COMINIO: ¿Para hablar con las calles?\n",
      "\n",
      "RODRIGO: ¿Quisiera ser la casa del miedo?\n",
      "\n",
      "EMILIA: ¿Estas cabezas? ¡Qué pie entonces, es cierto?\n",
      "\n",
      "MACBETH: No mirar eso. Si es que ellos te habría hecho\n",
      "más de que está aquí, porque lo hacen a un pensamiento\n",
      "que aquí a mi madre alcanzarán la mejor salta.\n",
      "\n",
      "MONTESCO: ¿Piensas esa prosticuión, señor?\n",
      "\n",
      "CASIO: Ahora,\n",
      "si tienes más que también mi alma.\n",
      "\n",
      "CORDELIA: ¡A venganza! ¿No es así!\n",
      "\n",
      "ANTONIO: ¿Qué? ¿Qué? ¿Qué hay de eso?\n",
      "\n",
      "CAFIS: Aquí está mi cargo, y ese traidor, si estuviera al asesinato abrigar\n",
      "su amor no puede ser en el padre. En marcha,\n",
      "que los pasos de ellos\n",
      "hay como ella lo cual, más cuenta. ¿Dónde está, señor?\n",
      "\n",
      "ROMEO: ¿Cómo está mi arrogante?\n",
      "\n",
      "SOLDADO: SEGUNDO Señor, a salvo, amor, ahí viene un poco.\n",
      "\n",
      "AAR:ÓN ¿Qué te estimas, pobre canalla de mi hermano?\n",
      "\n",
      "MACBETH: Adamos esta noche a la cabeza al ruin, y su anillo\n",
      "en tu maldición terrible, y contien de mala,\n",
      "para haber encontrado la verdad como el consejo\n",
      "y engaño esta noche alegre,\n",
      "y en medio mortificar eso así lo amante.\n",
      "Si no se han podido en estar canalla,\n",
      "y en mi muerte he visto en estes preparativos de la casa\n",
      "y esta mañana es la muerte entera.\n",
      "Esto en la piedad despidiende a su padre.\n",
      "Ahora nada era un pecado.\n",
      "\n",
      "Entran ANTONIO y MENAS.\n",
      "\n",
      "CABALLERO: Saluda, yo no sé lo que en tus militar no es solo usted,\n",
      "y solo sea sobre el pueblo. Su hermano, el cielo no encuentra\n",
      "la medida de una misma ciera perdida.\n",
      "\n",
      "Se sientan .\n",
      "\n",
      "SOLDADO: PRIMERO No más que mucho que se mostraría la mano y la mano.\n",
      "\n",
      "CLEOPATRA: Si está pobre. ¿Quién muere en esta espada?\n",
      "¿Conservad qué pasa?\n",
      "\n",
      "LUCIO: ¿Qué dice esa mirada?\n",
      "\n",
      "CASIO: ¡Ahorturado!\n",
      "\n",
      "SEGUNDO: ASESINO ¿Por qué hay alguno, señor, por qué te alimentaras?\n",
      "\n",
      "MARCIO: Seguid cómo es medor. ¡A menos dónde está más\n",
      "que a mi capitán, mi despedido! Al privó estos\n",
      "mar hacia todo lo mismo; y te ha hecho así\n",
      "mala alabanza al destierro. Esto era aquí en su marido.\n",
      "Algún soldado, en la tierra, que su madre habría pensamiento.\n",
      "\n",
      "CAPULETO: No, mi señora,\n",
      "solo algo más que esa palabra. Si el mundo sus perderías,\n",
      "ahizo el arrastre mal en sus acciones.\n",
      "\n",
      "Sale .\n",
      "\n",
      "EDMOND: Nada de vuestra pena.\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA III\n",
      "\n",
      "Entra FLAMINIO.\n",
      "¿Dónde está la amistad por ello?\n",
      "¿Quién le dio más que ahora el despertar\n",
      "de mi muerte enferma a la sola casa?\n",
      "\n",
      "MENSAJERO: El cuerpo es la mucho que desenvainará\n",
      "el puño y cualestial deseo y a una vez;\n",
      "por lo mejor que el compasión de esta sangre mortal\n",
      "puede dar soldado, y la cabeza estáis allá\n",
      "y los circunstante, el mismo de mi amada.\n",
      "\n",
      "MACBETH: ¿Por qué me atrevo a cual especiente mando\n",
      "que el pensamiento del mar, en eso se decidía\n",
      "de este cambio en la propusión completo?\n",
      "\n",
      "LAERTES: ¿Qué desalía?\n",
      "\n",
      "CASIO: ¡Ah, si este anciano con su marido!\n",
      "\n",
      "SEGUNDO: CABALLERO ¿Qué te ha hecho?\n",
      "\n",
      "SIRVIENTE: CAPULETO ¿Qué pasa, señor?\n",
      "\n",
      "LADY: MACBETH ¡Ah, dioses!\n",
      "\n",
      "Sale .\n",
      "Entra el SERVILIO.\n",
      "¿Qué es lo traicionero?\n",
      "\n",
      "CAPULETO: ¿Cómo está esa salud en el ardor?\n",
      "\n",
      "ROMEO: Adiós a mal solo así, y si te hago más que un hombre con ella.\n",
      "\n",
      "MARCIO: ¡Oh, sol esposa, cuidado, esposa, mi dueño!\n",
      "\n",
      "CAPIT:ÁN ¿Por qué hay más distinguina? ¿Qué pasa, esposo?\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA VI\n",
      "\n",
      "A la cama y apeten a su señoría en el cascilato de Antonio.\n",
      "\n",
      "TODOS: No se la causa pronto perfecta. Ahí está esperando\n",
      "a mí como les daré la misma culpa. ¿Puede que la vez,\n",
      "como una amante estás envolturado al corazón,\n",
      "y, de su mejor ser con las arrugas de su muerte?\n",
      "\n",
      "RODRIGO: ¿Es una vez más?\n",
      "\n",
      "ANTONIO: ¡Se habría de salir!\n",
      "\n",
      "Entran CÉSAR, LAVINIA, CABALLERO, los soldados .\n",
      "\n",
      "ENOBARBO: Así es, y le dio unido en el corazón\n",
      "en salvarme a una curididad y arro\n"
     ]
    }
   ],
   "source": [
    "output = sample(best_model, 10000,top_k=5, prime='¿Qué ta chendo? ¡Sexo,sexo!')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "with open('sample_gustavo.txt', 'w') as f:\n",
    "    f.write(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}