{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIMER ACTO\n",
      "\n",
      "\n",
      "ESCENA I\n",
      "\n",
      "Fanfarria . Entran arriba los tribunos y los senadores; luego, abajo, por\n",
      "una puerta SATURNINO y sus seguidores y por otra BASIANO y los\n",
      "suyos, con tambores y estandartes .\n",
      "\n",
      "SATURNINO: (A sus seguidores .)\n",
      "Nobles patricios, protectores de mis derechos,\n",
      "defiendan la verdad de mi causa con las armas;\n",
      "y ustedes, compatriotas, mis amados seguidores,\n",
      "aboguen por mi t√≠tulo de suc\n"
     ]
    }
   ],
   "source": [
    "with open('final_dataset.txt') as file:\n",
    "    TEXT = file.read()\n",
    "\n",
    "print(TEXT[:400])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "EPOCHS = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make, // integer division, round down\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "\n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows, n. of first row is the batch size, the other lenght is inferred\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    one_hot_array = np.zeros((np.multiply(*arr.shape),n_labels),dtype=np.float32)\n",
    "\n",
    "    one_hot_array[np.arange(one_hot_array.shape[0]),arr.flatten()] = 1.\n",
    "\n",
    "    one_hot_array = one_hot_array.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "chars = tuple(set(TEXT))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in TEXT])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[56 81 74  6 93 81 79 68 91 36]\n",
      " [59 28 76 32 79 53 60 76 32 94]\n",
      " [79  5 94  2 85 60  4 35 25 47]\n",
      " [94 32 76  8 40 94 92  2  5 79]\n",
      " [79 25 94 32 79  7 28 62  8 10]\n",
      " [38 62 79 62 25 79 28 62 38 94]\n",
      " [94 28 58 60 76 79 32 76 79  4]\n",
      " [ 8 76  8 47 94 79 68  7 28 47]]\n",
      "\n",
      "y\n",
      " [[81 74  6 93 81 79 68 91 36 73]\n",
      " [28 76 32 79 53 60 76 32 94 32]\n",
      " [ 5 94  2 85 60  4 35 25 47 28]\n",
      " [32 76  8 40 94 92  2  5 79 25]\n",
      " [25 94 32 79  7 28 62  8 10 47]\n",
      " [62 79 62 25 79 28 62 38 94  8]\n",
      " [28 58 60 76 79 32 76 79  4 60]\n",
      " [76  8 47 94 79 68  7 28 47 35]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4,\n",
    "                 drop_prob=0.5, lr=0.001,batch_size=64):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.train_on_gpu = torch.cuda.is_available()\n",
    "        self.epochs_trained=0\n",
    "\n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        self.encoded = np.array([self.char2int[ch] for ch in TEXT])\n",
    "\n",
    "    ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (self.train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "    def model_train(self, epochs=1, seq_length=50, clip=5, val_frac=0.1, print_every=10,print_state=True):\n",
    "        ''' Training a network\n",
    "\n",
    "            Arguments\n",
    "            ---------\n",
    "\n",
    "            net: CharRNN network\n",
    "            data: text data to train the network\n",
    "            epochs: Number of epochs to train\n",
    "            batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "            seq_length: Number of character steps per mini-batch\n",
    "            lr: learning rate\n",
    "            clip: gradient clipping\n",
    "            val_frac: Fraction of data to hold out for validation\n",
    "            print_every: Number of steps for printing training and validation loss\n",
    "\n",
    "        '''\n",
    "        loss_arr = []\n",
    "        val_loss_arr = []\n",
    "        if(self.train_on_gpu):\n",
    "            self.cuda()\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # create training and validation data\n",
    "        val_idx = int(len(self.encoded)*(1-val_frac))\n",
    "        data, val_data = self.encoded[:val_idx], self.encoded[val_idx:]\n",
    "\n",
    "\n",
    "\n",
    "        counter = 0\n",
    "        n_chars = len(self.chars)\n",
    "        for e in range(epochs):\n",
    "            # initialize hidden state\n",
    "            h = self.init_hidden(self.batch_size)\n",
    "\n",
    "            for x, y in get_batches(data, self.batch_size, seq_length):\n",
    "                counter += 1\n",
    "\n",
    "                # One-hot encode our data and make them Torch tensors\n",
    "                x = one_hot_encode(x, n_chars)\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                if(self.train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda().type(torch.cuda.LongTensor)\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                h = tuple([each.data for each in h])\n",
    "\n",
    "                # zero accumulated gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                # get the output from the model\n",
    "                output, h = self(inputs, h)\n",
    "\n",
    "                # calculate the loss and perform backprop\n",
    "                loss = criterion(output.cuda(), targets.view(self.batch_size*seq_length))\n",
    "                loss.backward()\n",
    "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "                opt.step()\n",
    "\n",
    "                # loss stats\n",
    "                if counter % print_every == 0 and print_state:\n",
    "                    # Get validation loss\n",
    "                    val_h = self.init_hidden(self.batch_size)\n",
    "                    val_losses = []\n",
    "                    self.eval()\n",
    "                    for x, y in get_batches(val_data, self.batch_size, seq_length):\n",
    "                        # One-hot encode our data and make them Torch tensors\n",
    "                        x = one_hot_encode(x, n_chars)\n",
    "                        x, y = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
    "\n",
    "                        # Creating new variables for the hidden state, otherwise\n",
    "                        # we'd backprop through the entire training history\n",
    "                        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                        inputs, targets = x, y\n",
    "                        if(self.train_on_gpu):\n",
    "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                        output, val_h = self(inputs, val_h)\n",
    "                        val_loss = criterion(output, targets.view(self.batch_size*seq_length).type(torch.cuda.LongTensor))\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                    self.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, self.epochs_trained),\n",
    "                          \"Step: {}...\".format(counter),\n",
    "                          \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                          \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                #val_loss_arr.append(val_loss.item())\n",
    "                loss_arr.append(loss.item())\n",
    "        return loss_arr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.loss = 100\n",
    "\n",
    "    def define_model(self,trial):\n",
    "        n_layers = trial.suggest_int(\"n_layers\",4,6)\n",
    "        n_hidden = trial.suggest_int(\"n_hidden\",312,712,step=100)\n",
    "        drop_prob = trial.suggest_float(\"drop_prob\",0.4,0.6,step=0.05)\n",
    "        lr = trial.suggest_float(\"lr\",0.001,0.01,step=0.001)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",16,128,step=16)\n",
    "\n",
    "        return CharRNN(chars, n_hidden=n_hidden, n_layers=n_layers, drop_prob=drop_prob, lr=lr,batch_size=batch_size)\n",
    "\n",
    "    def objective(self,trial):\n",
    "        model = self.define_model(trial).to('cuda')\n",
    "        loss = 0\n",
    "        status = 'Current Params:\\n'\n",
    "        #print('Current Params:',end='\\r')\n",
    "        for key, value in trial.params.items():\n",
    "            status+=\"    {}: {}\".format(key, value) + '\\n'\n",
    "            #print(\"    {}: {}\".format(key, value),end='\\r')\n",
    "        print(status,end='\\r')\n",
    "\n",
    "        for epoch in tnrange(EPOCHS,desc='Internal Epochs'):\n",
    "            #print(\" ABSOLUTE EPOCH:   \"+str(epoch))\n",
    "            #print(\"     Relative Epochs:    \" + str(model.epochs_trained))\n",
    "            loss_arr= model.model_train(seq_length=160,print_state=False)\n",
    "            loss = loss_arr[0]\n",
    "            print(\"         Current Loss: %f\"%loss,end='\\r')\n",
    "            trial.report(loss,epoch)\n",
    "\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if loss < self.loss:\n",
    "            self.loss = loss\n",
    "            self.model = model\n",
    "        return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 18:20:12,295]\u001B[0m A new study created in memory with name: no-name-363f8683-9283-4ee4-8502-679f2c2a4473\u001B[0m\n",
      "C:\\ProgramData\\Anaconda3\\envs\\robo_shakespeare\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "636eabb91e3843918083b02803f49717"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.5\n",
      "    lr: 0.004\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1469911fcb84434dac186682e11da74c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 19:03:29,202]\u001B[0m Trial 0 finished with value: 3.285987377166748 and parameters: {'n_layers': 6, 'n_hidden': 612, 'drop_prob': 0.5, 'lr': 0.004, 'batch_size': 48}. Best is trial 0 with value: 3.285987377166748.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.008\n",
      "    batch_size: 96\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1992c897f47f45c4ae12974b32f61f41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 19:47:46,741]\u001B[0m Trial 1 finished with value: 3.30895733833313 and parameters: {'n_layers': 6, 'n_hidden': 612, 'drop_prob': 0.45, 'lr': 0.008, 'batch_size': 96}. Best is trial 0 with value: 3.285987377166748.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.01\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a19967b261ce4fe6afbd01ba54031332"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 20:27:34,205]\u001B[0m Trial 2 finished with value: 3.2844347953796387 and parameters: {'n_layers': 5, 'n_hidden': 612, 'drop_prob': 0.45, 'lr': 0.01, 'batch_size': 48}. Best is trial 2 with value: 3.2844347953796387.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.002\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4674b8e5927f4d9e99d721c83db3768b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 20:54:50,506]\u001B[0m Trial 3 finished with value: 0.9168450236320496 and parameters: {'n_layers': 4, 'n_hidden': 712, 'drop_prob': 0.4, 'lr': 0.002, 'batch_size': 32}. Best is trial 3 with value: 0.9168450236320496.\u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 512\n",
      "    drop_prob: 0.5\n",
      "    lr: 0.008\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fdd8840ab90441caee42658345d4bdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:07:01,541]\u001B[0m Trial 4 finished with value: 3.2884576320648193 and parameters: {'n_layers': 5, 'n_hidden': 512, 'drop_prob': 0.5, 'lr': 0.008, 'batch_size': 48}. Best is trial 3 with value: 0.9168450236320496.\u001B[0m Loss: 3.288988         Current Loss: 3.289417\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.55\n",
      "    lr: 0.01\n",
      "    batch_size: 80\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57c9115004344b50b444045c14cfc7ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:07:30,866]\u001B[0m Trial 5 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.6\n",
      "    lr: 0.002\n",
      "    batch_size: 80\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b1370d6efe1439d94b14ea0cb1ea8f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:08:07,430]\u001B[0m Trial 6 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 512\n",
      "    drop_prob: 0.6\n",
      "    lr: 0.008\n",
      "    batch_size: 16\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c11da5cc43f45d3bc81369ff7a8b63f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:08:55,152]\u001B[0m Trial 7 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 6\n",
      "    n_hidden: 312\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.008\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff39dc2511644c87b769c7036577c52c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:09:28,126]\u001B[0m Trial 8 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 512\n",
      "    drop_prob: 0.6\n",
      "    lr: 0.01\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d85932fd0d75479f8a193344ca16330d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:10:03,375]\u001B[0m Trial 9 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.001\n",
      "    batch_size: 128\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79279122d71843acb94a82d258fcd357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:10:29,688]\u001B[0m Trial 10 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.004\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3871051cbde042a49017afb17aa1d43e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:10:58,911]\u001B[0m Trial 11 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.005\n",
      "    batch_size: 16\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccb2da1170e34198aa7e4e68b1fd536d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:12:27,012]\u001B[0m Trial 12 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 412\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.003\n",
      "    batch_size: 64\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c06db6562e148d9b99df20707c46abe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:12:36,733]\u001B[0m Trial 13 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.006\n",
      "    batch_size: 32\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af6b1435ab3045248aab53150a3e3284"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:13:09,680]\u001B[0m Trial 14 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.006\n",
      "    batch_size: 64\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88b5bfe03b024407a870a3e77e642048"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:13:41,131]\u001B[0m Trial 15 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 412\n",
      "    drop_prob: 0.45\n",
      "    lr: 0.001\n",
      "    batch_size: 96\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dce6ba2b8fae4e2fb3e57df5ad53f157"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:13:50,540]\u001B[0m Trial 16 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.5\n",
      "    lr: 0.003\n",
      "    batch_size: 16\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ba68ab30ca940b3bdc71d783908e70e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:15:42,009]\u001B[0m Trial 17 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 5\n",
      "    n_hidden: 612\n",
      "    drop_prob: 0.55\n",
      "    lr: 0.007\n",
      "    batch_size: 128\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e827fb097a44bf1addde2dc3b7d52f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:16:10,288]\u001B[0m Trial 18 pruned. \u001B[0m\n",
      "Current Params:\n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.009000000000000001\n",
      "    batch_size: 48\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": "Internal Epochs:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d259a519801d448c822e5d9fa8c1dcff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-05-29 21:17:19,284]\u001B[0m Trial 19 pruned. \u001B[0m\n",
      "Study statistics: \n",
      "  Number of finished trials:  20\n",
      "  Number of pruned trials:  15\n",
      "  Number of complete trials:  5\n",
      "Best trial:\n",
      "  Value:  0.9168450236320496\n",
      "  Params: \n",
      "    n_layers: 4\n",
      "    n_hidden: 712\n",
      "    drop_prob: 0.4\n",
      "    lr: 0.002\n",
      "    batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluator = Objective()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(evaluator.objective,n_trials=20,show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(98, 712, num_layers=4, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=712, out_features=98, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "best_model = evaluator.model\n",
    "\n",
    "print(best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "n_hidden 612\n",
    "drop_prob 0.6\n",
    "lr 0.003"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "model_shakespeare_tragedias = 'rnn_50_epoch_4-712_batch-32_lr-002.net'\n",
    "\n",
    "checkpoint = {'n_hidden': best_model.n_hidden,\n",
    "              'n_layers': best_model.n_layers,\n",
    "              'state_dict': best_model.state_dict(),\n",
    "              'tokens': best_model.chars}\n",
    "\n",
    "with open(model_shakespeare_tragedias, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "    ''' Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "    '''\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    # apply softmax to get p probabilities for the likely next character giving x\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu() # move to cpu\n",
    "\n",
    "    # get top characters\n",
    "    # considering the k most probable characters with topk method\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return net.int2char[char], h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def sample(net, size, prime='PRIMER ACTO', top_k=None):\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval() # eval mode\n",
    "\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øQu√© ta chendo? ¬°Sexo,sexo! ¬°Ahora que\n",
      "este es el mal con las piedras que mi antiguo encuentro\n",
      "para ser servido!\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA VII\n",
      "\n",
      "Un palacio.\n",
      "Entran LENNOX, ROSS\n",
      "y AAR√ìN, cuanto la puerta .\n",
      "\n",
      "Sale .\n",
      "\n",
      "COMINIO: ¬øQui√©n es el dolor de su se√±or?\n",
      "\n",
      "ANTONIO: As√≠ es ah√≠ tienes.\n",
      "\n",
      "SEGUNDA: BRUJA ¬°Maldito se acerca!\n",
      "\n",
      "Salen el M√âDICO y CAFIS.\n",
      "\n",
      "ANTONIO: ¬°Ay, estas se√±al e informes!\n",
      "\n",
      "SOLDADO: SEGUNDO No, dijo que se alimentar√° pensarlo.\n",
      "Santo espeso del cielo, este amor que se la habr√≠a sucedido.\n",
      "\n",
      "LADY: MACBETH ¬øD√≥nde est√° el modo? ¬°Socorro!\n",
      "¬øNo es la verdad de mi amigo, como linaje,\n",
      "seg√∫n se estrecha en la tercera cabeza? ¬øQu√© hace, y su amigo?\n",
      "\n",
      "ROMEO: ¬øQu√©? ¬øPor qu√© natural donde el mismo\n",
      "dulce sabidur√≠a en su curso\n",
      "el poder derribero en mi muerte a las cuerpas? ¬øC√≥mo es un asesinato\n",
      "que as√≠ soy la cama el punto de esos palabras\n",
      "como un monto, con medio de los cuernos, con las prostiguidas\n",
      "en la nada, cual caballos camino aqu√≠ con la muerte?\n",
      "¬øNo te he vivido esos agravios alcanzados\n",
      "al menos capacer como un saludo de su padre\n",
      "y como el agua de la maldad se enviar√≠a?\n",
      "\n",
      "CORDELIA: ¬øC√≥mo, cu√°ndo empez√≥?\n",
      "\n",
      "Sale el CABALLERO.\n",
      "¬øQu√© hizo particular de la verdigo?\n",
      "\n",
      "LEAR: No.\n",
      "\n",
      "ANTONIO: ¬°Ah, sin darle a m√≠! ¬øCon qu√© me presenta la trampa?\n",
      "\n",
      "ANTONIO: No lo eristar√≠as.\n",
      "Estoy de loco, se√±or m√≠o,\n",
      "sin duda alguna de tu amor presente.\n",
      "\n",
      "ANTONIO: ¬°Mientras lo tienes a su amor!\n",
      "\n",
      "EMILIA: Se√±or, se√±or, solo sufres de lugar y cuerno desde la muerte\n",
      "es estrechamente.\n",
      "\n",
      "LUCILIO: A tu se√±ora me perden a esto, pero es el coraz√≥n que doy, y se dice\n",
      "un confundido cual a las calles.\n",
      "¬øEst√° sabido? ¬°Oh, males, seguro!\n",
      "\n",
      "ALBANY: Aqu√≠ estoy, bien.\n",
      "\n",
      "CHARMIAN: No, si te has confiar√° esa propia pataniza, no es m√°s de que te\n",
      "conozco. ¬øQu√© se arrastrar√° de un miedo exparchar√°s e incendicionero,\n",
      "para arrancar su castigo?\n",
      "\n",
      "SOLDADO: As√≠ sea.\n",
      "Mira el mar y eso.\n",
      "\n",
      "ENOBARBO: ¬øC√≥mo? ¬øPor qu√© no te he o√≠do?\n",
      "\n",
      "SIRVIENTE: DE FILOTO ¬øC√≥mo? ¬øNo es la mista muerte del cielo\n",
      "que las preservas est√°n aqu√≠, a su sangre?\n",
      "¬øQu√© sucede? ¬°Ay, despachado!\n",
      "\n",
      "Sale CORDELIA.\n",
      "\n",
      "Entra CLEOPATRA.\n",
      "¬øY si este mar me propongo?\n",
      "\n",
      "BRUTO: ¬øEllo te habr√≠a cometido a los hombres?\n",
      "Entre el dulce modo en mi acaso siga a su sangre.\n",
      "¬øNo pueden acu√±ir a sus cartos?\n",
      "\n",
      "SENADOR: PRIMERO ¬øPuede ahora?\n",
      "\n",
      "APEMANTO: ¬°Oh, se√±ora, encontrar√°s estos preparativos dispuestos!\n",
      "¬°Oh, maldito es el mejor cenal en una corona?\n",
      "\n",
      "CAPULETO: ¬øQue se acorrear√° en esto as√≠? Ah√≠ est√°\n",
      "si en tus cabezas me parecen para que tengo esperanza.\n",
      "\n",
      "LUCIO: Aqu√≠, se√±or, el cielo el de la parte de su padre.\n",
      "Sus hijos alconcrean con sus mujeres y sacrificios\n",
      "de su parte, a la cual se enfr√≠a aqu√≠.\n",
      "\n",
      "LEAR: ¬øEs este ese el padre de C√©sar?\n",
      "\n",
      "BALTASAR: ¬°Pobre Tom!\n",
      "\n",
      "BRUTO: No me hab√≠a honro. ¬øDe eso has escapado as√≠?\n",
      "¬øQu√© pretende siempre? ¬°Oh, muerte, sus angustias!\n",
      "¬øPues es por lo que este tirano malas muertos?\n",
      "\n",
      "ROMEO: A m√≠ no me da algo. ¬°Ah, qu√© digno de secuel\n",
      "despierta.\n",
      "Es este mar deber de tonto presto.\n",
      "\n",
      "Le dura con las puertas .\n",
      "\n",
      "ROSENCRANTZ: ¬øDe veras? No, no te ataremos.\n",
      "\n",
      "SEGUNDO: SENADOR ¬øEs que no te ata√±as?\n",
      "\n",
      "AAR:√ìN ¬øQui√©n me ha hecho ma√±ana?\n",
      "\n",
      "SOLDADO: PRIMERO Si les encuentra lujuria, para cambiado disfruto.\n",
      "C√âSAR ¬øY si es como la puerta?\n",
      "\n",
      "CASIO: ¬øNo te aconsejo eso?\n",
      "¬øCallar a ustedes, mi buen se√±or?\n",
      "¬øPor qu√© hemos sufrido contra m√≠ el duque?\n",
      "\n",
      "ANTONIO: ¬øQu√© pasa?\n",
      "\n",
      "SIRVIENTE: DE VARR√ìN No, se√±ora.\n",
      "\n",
      "POLONIO: S√≠; es el cepo. ¬°Qu√© pie a esto!\n",
      "\n",
      "Sale .\n",
      "Entra LUCIANO.\n",
      "\n",
      "ANTONIO: ¬øCon eso?\n",
      "\n",
      "CLEOPATRA: ¬°Adi√≥s! Ahora vuestra especie de los hermanos.\n",
      "Pero si lo que son enseguidas. ¬øQu√© hay?\n",
      "\n",
      "CORNWALL: ¬°Adi√≥s!\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA VI\n",
      "\n",
      "Entran los se√±ores, por campos de sus amomentos cuando el cuerpo de Antonio .\n",
      "\n",
      "CLITO: Ahora te haga aqu√≠.\n",
      "\n",
      "CASIO: No, milord.\n",
      "\n",
      "HAMLET: Se√±or, m√°s que la muerte\n",
      "del asunto este conflicto\n",
      "que ha estado en ellos. Se√±or m√≠o,\n",
      "que nuestro amor nos propondr√°\n",
      "que ense√±arle en esto antes que lo hondo es cierto,\n",
      "sino desde que el mucho que escap√≥ del mundo,\n",
      "que no era el dios, y a mi alma y mis cartas\n",
      "cuando la separaba alcanzan\n",
      "los ojos; y lo que se acerca\n",
      "con su estrella disputa en su perro y circunstancia.\n",
      "¬øQu√© es lo que es esto?\n",
      "\n",
      "SATURNINO: (Aparte .) ¬°Ah, qu√©, amigo y ver ahora\n",
      "el deseo que las venganza!\n",
      "\n",
      "CASIO: ¬øQu√© pasa?\n",
      "\n",
      "CALPURNIA: ¬øQuise ser suya?\n",
      "\n",
      "SOLDADO: SEGUNDO ¬øCu√°nto hace que no es m√°s que tu padre,\n",
      "que no es as√≠?\n",
      "Soy una cara m√°s perder√≠a preferir√≠a m√≠ el amor\n",
      "por ella al cuerpo de listo, y estas alegres\n",
      "maldiciones y les escuento a sus pasiones,\n",
      "y a los deseos, de su poder en tierra.\n",
      "¬øQu√© sucede?\n",
      "\n",
      "RODRIGO: No enviar√© a llegar.\n",
      "\n",
      "LEAR: ¬°Que tu se√±or me alabe, con la campana de las tierra!\n",
      "\n",
      "SE:√ëORA CAPULETO ¬øQui√©n me arrodillo? ¬øC√≥mo est√°s?\n",
      "\n",
      "CLEOPATRA: S√≠, por mi amo de ser tratas,\n",
      "y esto muy bien perderoso,\n",
      "ya que el de la corte est√° muerta. Su honor me despido.\n",
      "Ser√°s le considero al marido en la castill√≥n.\n",
      "\n",
      "Se arramiaba .\n",
      "¬°Algo!\n",
      "\n",
      "Entra CASIO, con un CRIADO.\n",
      "Ah, se√±ora.\n",
      "\n",
      "CLEOPATRA: No es ponderezcado este anciano.\n",
      "¬øQu√© has visto?\n",
      "\n",
      "ANTONIO: No hice algo de mala serio.\n",
      "¬øNo te esconde en m√≠, el de mis dos?\n",
      "\n",
      "MACBETH: ¬°Ah, se√±or, es este miedo escapa y de ella\n",
      "y el primero, y espero de su cravesta!\n",
      "¬øPero eres mi amor este contento m√°s pobre?\n",
      "\n",
      "ROMEO: S√≠, mi amo.\n",
      "\n",
      "CLEOPATRA: Se√±ora. ¬°Qu√© su medicina!\n",
      "Si hubiera sido ma√±ana est√° mi madre,\n",
      "y a tu madre sean las tiendas,\n",
      "y se aposta a estos maldecirios.\n",
      "\n",
      "CLEOPATRA: ¬øCu√°l es mi amo del turco?\n",
      "\n",
      "CLEOPATRA: ¬øQu√© perdi√≥ a mi alma, que no hubiese\n",
      "de senter aqu√≠ a su mujer?\n",
      "\n",
      "CLEOPATRA: ¬°Salve, escuche el cielo sus asesinos,\n",
      "y el papel que solo es lo mejor,\n",
      "y la circunstancia se envenen√≥ a mi padre\n",
      "y a tu muralla es m√°s que el del cielo;\n",
      "pero su pellego hace a matarme.\n",
      "\n",
      "ANTONIO: Ahora venga a mi apresura.\n",
      "¬øQue nos dec√≠stelo en su pensar?\n",
      "\n",
      "SEGUNDO: ACTO\n",
      "\n",
      "\n",
      "ESCENA I\n",
      "\n",
      "Entra FRAY LORENZO.\n",
      "\n",
      "PRIMER: ASESINO Se√±ora.\n",
      "\n",
      "CLEOPATRA: ¬°Callad, se√±or, para que se te ha dejo\n",
      "alg√∫n amargo, despu√©s, aunque el desprecio honrado\n",
      "descubre a un solo padre en mi contallente\n",
      "empalatante escoltar a C√©sar!\n",
      "\n",
      "TITO: ¬øQue me atreves, cobra a ser el aguij√≥n, ese mejor que todo\n",
      "acampar a su alcoba o con la pestilente de los amos? El modo soporto\n",
      "el mundo en la trampa de la presencia de mi se√±ora.\n",
      "\n",
      "CORNWALL: As√≠ como la marea de la mirada de los ojos.\n",
      "\n",
      "CHARMIAN: No hay amigos m√≠os, eso, despu√©s de este hombre.\n",
      "Cuando est√© es mi propia sangre.\n",
      "¬øPor qu√© el mundo ha dado el maldito casado?\n",
      "¬øQu√© es esto son los hombres, cuando se estremece tiempo?\n",
      "¬øD√≥nde va con los ojos?\n",
      "\n",
      "CORNWALL: Sin embargo, me perdido.\n",
      "¬øD√≥nde tienes cuernos de mis campanas?\n",
      "\n",
      "COMINIO: ¬øPara hablar con las calles?\n",
      "\n",
      "RODRIGO: ¬øQuisiera ser la casa del miedo?\n",
      "\n",
      "EMILIA: ¬øEstas cabezas? ¬°Qu√© pie entonces, es cierto?\n",
      "\n",
      "MACBETH: No mirar eso. Si es que ellos te habr√≠a hecho\n",
      "m√°s de que est√° aqu√≠, porque lo hacen a un pensamiento\n",
      "que aqu√≠ a mi madre alcanzar√°n la mejor salta.\n",
      "\n",
      "MONTESCO: ¬øPiensas esa prosticui√≥n, se√±or?\n",
      "\n",
      "CASIO: Ahora,\n",
      "si tienes m√°s que tambi√©n mi alma.\n",
      "\n",
      "CORDELIA: ¬°A venganza! ¬øNo es as√≠!\n",
      "\n",
      "ANTONIO: ¬øQu√©? ¬øQu√©? ¬øQu√© hay de eso?\n",
      "\n",
      "CAFIS: Aqu√≠ est√° mi cargo, y ese traidor, si estuviera al asesinato abrigar\n",
      "su amor no puede ser en el padre. En marcha,\n",
      "que los pasos de ellos\n",
      "hay como ella lo cual, m√°s cuenta. ¬øD√≥nde est√°, se√±or?\n",
      "\n",
      "ROMEO: ¬øC√≥mo est√° mi arrogante?\n",
      "\n",
      "SOLDADO: SEGUNDO Se√±or, a salvo, amor, ah√≠ viene un poco.\n",
      "\n",
      "AAR:√ìN ¬øQu√© te estimas, pobre canalla de mi hermano?\n",
      "\n",
      "MACBETH: Adamos esta noche a la cabeza al ruin, y su anillo\n",
      "en tu maldici√≥n terrible, y contien de mala,\n",
      "para haber encontrado la verdad como el consejo\n",
      "y enga√±o esta noche alegre,\n",
      "y en medio mortificar eso as√≠ lo amante.\n",
      "Si no se han podido en estar canalla,\n",
      "y en mi muerte he visto en estes preparativos de la casa\n",
      "y esta ma√±ana es la muerte entera.\n",
      "Esto en la piedad despidiende a su padre.\n",
      "Ahora nada era un pecado.\n",
      "\n",
      "Entran ANTONIO y MENAS.\n",
      "\n",
      "CABALLERO: Saluda, yo no s√© lo que en tus militar no es solo usted,\n",
      "y solo sea sobre el pueblo. Su hermano, el cielo no encuentra\n",
      "la medida de una misma ciera perdida.\n",
      "\n",
      "Se sientan .\n",
      "\n",
      "SOLDADO: PRIMERO No m√°s que mucho que se mostrar√≠a la mano y la mano.\n",
      "\n",
      "CLEOPATRA: Si est√° pobre. ¬øQui√©n muere en esta espada?\n",
      "¬øConservad qu√© pasa?\n",
      "\n",
      "LUCIO: ¬øQu√© dice esa mirada?\n",
      "\n",
      "CASIO: ¬°Ahorturado!\n",
      "\n",
      "SEGUNDO: ASESINO ¬øPor qu√© hay alguno, se√±or, por qu√© te alimentaras?\n",
      "\n",
      "MARCIO: Seguid c√≥mo es medor. ¬°A menos d√≥nde est√° m√°s\n",
      "que a mi capit√°n, mi despedido! Al priv√≥ estos\n",
      "mar hacia todo lo mismo; y te ha hecho as√≠\n",
      "mala alabanza al destierro. Esto era aqu√≠ en su marido.\n",
      "Alg√∫n soldado, en la tierra, que su madre habr√≠a pensamiento.\n",
      "\n",
      "CAPULETO: No, mi se√±ora,\n",
      "solo algo m√°s que esa palabra. Si el mundo sus perder√≠as,\n",
      "ahizo el arrastre mal en sus acciones.\n",
      "\n",
      "Sale .\n",
      "\n",
      "EDMOND: Nada de vuestra pena.\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA III\n",
      "\n",
      "Entra FLAMINIO.\n",
      "¬øD√≥nde est√° la amistad por ello?\n",
      "¬øQui√©n le dio m√°s que ahora el despertar\n",
      "de mi muerte enferma a la sola casa?\n",
      "\n",
      "MENSAJERO: El cuerpo es la mucho que desenvainar√°\n",
      "el pu√±o y cualestial deseo y a una vez;\n",
      "por lo mejor que el compasi√≥n de esta sangre mortal\n",
      "puede dar soldado, y la cabeza est√°is all√°\n",
      "y los circunstante, el mismo de mi amada.\n",
      "\n",
      "MACBETH: ¬øPor qu√© me atrevo a cual especiente mando\n",
      "que el pensamiento del mar, en eso se decid√≠a\n",
      "de este cambio en la propusi√≥n completo?\n",
      "\n",
      "LAERTES: ¬øQu√© desal√≠a?\n",
      "\n",
      "CASIO: ¬°Ah, si este anciano con su marido!\n",
      "\n",
      "SEGUNDO: CABALLERO ¬øQu√© te ha hecho?\n",
      "\n",
      "SIRVIENTE: CAPULETO ¬øQu√© pasa, se√±or?\n",
      "\n",
      "LADY: MACBETH ¬°Ah, dioses!\n",
      "\n",
      "Sale .\n",
      "Entra el SERVILIO.\n",
      "¬øQu√© es lo traicionero?\n",
      "\n",
      "CAPULETO: ¬øC√≥mo est√° esa salud en el ardor?\n",
      "\n",
      "ROMEO: Adi√≥s a mal solo as√≠, y si te hago m√°s que un hombre con ella.\n",
      "\n",
      "MARCIO: ¬°Oh, sol esposa, cuidado, esposa, mi due√±o!\n",
      "\n",
      "CAPIT:√ÅN ¬øPor qu√© hay m√°s distinguina? ¬øQu√© pasa, esposo?\n",
      "\n",
      "Salen .\n",
      "\n",
      "ESCENA VI\n",
      "\n",
      "A la cama y apeten a su se√±or√≠a en el cascilato de Antonio.\n",
      "\n",
      "TODOS: No se la causa pronto perfecta. Ah√≠ est√° esperando\n",
      "a m√≠ como les dar√© la misma culpa. ¬øPuede que la vez,\n",
      "como una amante est√°s envolturado al coraz√≥n,\n",
      "y, de su mejor ser con las arrugas de su muerte?\n",
      "\n",
      "RODRIGO: ¬øEs una vez m√°s?\n",
      "\n",
      "ANTONIO: ¬°Se habr√≠a de salir!\n",
      "\n",
      "Entran C√âSAR, LAVINIA, CABALLERO, los soldados .\n",
      "\n",
      "ENOBARBO: As√≠ es, y le dio unido en el coraz√≥n\n",
      "en salvarme a una curididad y arro\n"
     ]
    }
   ],
   "source": [
    "output = sample(best_model, 10000,top_k=5, prime='¬øQu√© ta chendo? ¬°Sexo,sexo!')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "with open('sample_gustavo.txt', 'w') as f:\n",
    "    f.write(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}